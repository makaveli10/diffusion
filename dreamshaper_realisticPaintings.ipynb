{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81317cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".inner_cell div.text_cell_render {\n",
       "    font-size: 125%;\n",
       "    overflow: none;\n",
       "    border-left: 10px solid #ff903b;\n",
       "    background: #ffe5d0;\n",
       "    margin-left: -15px;\n",
       "    padding-left: 15px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pylab\n",
    "from smlai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b0d38",
   "metadata": {},
   "source": [
    "## Dreamshaper\n",
    "This is a model to make good portraits that do not look like cg or photos with heavy filters, but more like actual paintings. Can do great backgrounds and anime style characters.\n",
    "\n",
    "- We use the checkpoint download link from the model at [civit-ai here](https://civitai.com/models/4384/dreamshaper)\n",
    "- ```load_civit_ckpt``` with the link and a model name to save the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996f6e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step key not found in model\n",
      "In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA weights (usually better for inference), please make sure to add the `--extract_ema` flag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ckpt_download_link = 'https://civitai.com/api/download/models/43888'\n",
    "ckpt_name = 'dreamshaper_5_baked_vae'\n",
    "model = load_civit_ckpt(ckpt_download_link, ckpt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ce611",
   "metadata": {},
   "source": [
    "**Paint some backgrounds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a643451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1eff4bd58be4b6bae11f31a92d763c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/G4hwycjXlE542Ntt_0mfEw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/G4hwycjXlE542Ntt_0mfEw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/G4hwycjXlE542Ntt_0mfEw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt='the start of something beautiful, digital, concept art, detailed illustration, 4k',\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9105b4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfce66244db4c7d9c7215ccf8492737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/x6pF5MUOFQGgPy4Nrkmxow==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/x6pF5MUOFQGgPy4Nrkmxow==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/x6pF5MUOFQGgPy4Nrkmxow==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with negativc prompt\n",
    "generate(\n",
    "    model,\n",
    "    prompt=\"the start of something beautiful, digital, concept art, detailed illustration, 4k, 8k\",\n",
    "    negative_prompt='3d, cartoon, lowres, bad anatomy, jpeg atrifact, blurry, ugly, low quality',\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9206477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dece9579a641431daf135b11702886ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 0.jpg\n",
      "\tdetect 0 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/K3LaUfM2oIfcl4njK5CvhQ==\n",
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 1.jpg\n",
      "\tdetect 0 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/K3LaUfM2oIfcl4njK5CvhQ==\n",
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 2.jpg\n",
      "\tdetect 0 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/K3LaUfM2oIfcl4njK5CvhQ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/K3LaUfM2oIfcl4njK5CvhQ==/final_results/0.png\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/K3LaUfM2oIfcl4njK5CvhQ==/final_results/1.png\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/K3LaUfM2oIfcl4njK5CvhQ==/final_results/2.png\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upscale images\n",
    "generate(\n",
    "    model,\n",
    "    prompt=\"the start of something beautiful, digital, concept art, detailed illustration, 4k, 8k\",\n",
    "    negative_prompt='3d, cartoon, lowres, bad anatomy, jpeg atrifact, blurry, ugly, low quality',\n",
    "    N=3,\n",
    "    upscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0cf9ee",
   "metadata": {},
   "source": [
    "A spacescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fee475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9be0f3e76e47ceb06643b7f19ff302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/WC_RILXID-9qOUeRNUIgYQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/WC_RILXID-9qOUeRNUIgYQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/WC_RILXID-9qOUeRNUIgYQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt=\"oil on matte canvas, scifi spacescape colony, intricate and highly detailed, rutkowski\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afda0293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f07f0dcc1ed48fbad56f21825e12386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/OhhfWzRs1YYpsb6IxYpIbw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/OhhfWzRs1YYpsb6IxYpIbw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/OhhfWzRs1YYpsb6IxYpIbw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with negative prompt\n",
    "generate(\n",
    "    model,\n",
    "    prompt='oil on matte canvas, scifi spacescape colony, intricate and highly detailed, rutkowski',\n",
    "    negative_prompt='3d, cartoon, lowres, bad anatomy, jpeg atrifact, blurry, ugly, low quality',\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f683fa",
   "metadata": {},
   "source": [
    "**Negative Prompt**\n",
    "\n",
    "- We can easily generate good images by iteratively improvising our prompt.\n",
    "- Negative prompt is clearly helpful in improving the quality and removing artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f2177c",
   "metadata": {},
   "source": [
    "### Detailed prompts\n",
    "Lets generate a high quality background with improvised prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2753a786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878621d382f3463c93c889c9cace36e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 0.jpg\n",
      "\tdetect 0 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/VlnLE_OeZkNX9YQPMOaB-g==\n",
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 1.jpg\n",
      "\tdetect 0 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/VlnLE_OeZkNX9YQPMOaB-g==\n",
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 2.jpg\n",
      "\tdetect 0 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/VlnLE_OeZkNX9YQPMOaB-g==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/VlnLE_OeZkNX9YQPMOaB-g==/final_results/0.png\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/VlnLE_OeZkNX9YQPMOaB-g==/final_results/1.png\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/VlnLE_OeZkNX9YQPMOaB-g==/final_results/2.png\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt='The sound of silence, detailed illustration, digital art, overdetailed art, \\\n",
    "        hd, 4k, Dan Mumford, Krzysztof Maziarz, trending on artstation,',\n",
    "    negative_prompt='3d, cartoon, lowres, bad anatomy, text, error, fewer digits, cropped, worst quality, \\\n",
    "        low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres',\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043e188",
   "metadata": {},
   "source": [
    "## Paint characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f9d114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f51fb8b35247bd9d2f29d3439ab9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/ktGbOzfFA0tNM0X2ohj7Tw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/ktGbOzfFA0tNM0X2ohj7Tw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/ktGbOzfFA0tNM0X2ohj7Tw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt=\"photorealistic photo of an evil hermit, masculine, villain\",\\\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ad195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6715ebceb4d7491c8a4ce240bec2556b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/3iSlhzzvRzxTlFhSenQ8xw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/3iSlhzzvRzxTlFhSenQ8xw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/3iSlhzzvRzxTlFhSenQ8xw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more details, improvising the prompt\n",
    "generate(\n",
    "    model,\n",
    "    prompt=\"photorealistic photo of an evil hermit, male, masculine, villain, medium and maroon hair, anti hero, \\\n",
    "        sinister, detailed face and body, professional oil painting, dramatic, cinematic\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f6fdf",
   "metadata": {},
   "source": [
    "**With negative prompt**\n",
    "\n",
    "With a negative prompt to remove artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33700c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2969a694185e43f5a384a6569fbafcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/uh6FuMsjlaZqf6I1gylhqQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/uh6FuMsjlaZqf6I1gylhqQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/uh6FuMsjlaZqf6I1gylhqQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets add some more face related keywords to remove artefacts from the above image\n",
    "generate(\n",
    "    model,\n",
    "    prompt=\"photorealistic photo of an evil hermit, male, masculine, villain, medium and maroon hair, anti hero, \\\n",
    "        sinister, detailed face and body, professional oil painting, dramatic, cinematic\",\n",
    "    negative_prompt=\"3d, cartoon, lowres, bad anatomy, text, error, fewer digits, cropped, worst quality, \\\n",
    "        low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, \\\n",
    "        bad art, poorly drawn face, body out of frame, mutated, extra limbs, extra legs, extra arms\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c4aca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1fd6a72e474868b83cc87f222d47fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/SJMwGwiS1M559UCZ0mF_nQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/SJMwGwiS1M559UCZ0mF_nQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/SJMwGwiS1M559UCZ0mF_nQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another character\n",
    "generate(\n",
    "    model,\n",
    "    prompt=\"photorealistic and cinematic scifi male armor, mandolorian helmet, scifi movie style, outdoors, \\\n",
    "        highest details, fine details\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d188fb",
   "metadata": {},
   "source": [
    "**With negative prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d229f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789149ef294943c38b06250d2adde6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/QgaSx1Pn3so_-L4zS3cHKw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/QgaSx1Pn3so_-L4zS3cHKw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/QgaSx1Pn3so_-L4zS3cHKw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt=\"photorealistic and cinematic scifi male armor, mandolorian helmet, scifi movie style, outdoors, \\\n",
    "        highest details, fine details, 8k, ultra hd\",\n",
    "    negative_prompt=\"3d, cartoon, lowres, bad anatomy, text, error, fewer digits, cropped, worst quality, \\\n",
    "        low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, \\\n",
    "        bad art, poorly drawn face, body out of frame, mutated, extra limbs, extra legs, extra arms\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa6e7da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9014adcda0429ab6afd81821590361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/bfQ6bpXuhGtJUFoQmQ9OaQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/bfQ6bpXuhGtJUFoQmQ9OaQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/bfQ6bpXuhGtJUFoQmQ9OaQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt=\"spiderman, hulk, 1man, photorealistic, cinematic, deviant and detailed art, high textures, \\\n",
    "        high resolution, dreamlikeart, 8k, highly detailed, digital art, realistic\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9593947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbc7c2070f7470bbe741a20ad5e9af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/e8BALxg7y-CxwzEt_VIz8w==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/e8BALxg7y-CxwzEt_VIz8w==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/e8BALxg7y-CxwzEt_VIz8w==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with detailed prompt and a negative prompt\n",
    "generate(\n",
    "    model,\n",
    "    prompt=\"spiderman, hulk, 1man, photorealistic, cinematic, deviant and detailed art, high textures, \\\n",
    "        high resolution, dreamlikeart, 8k, highly detailed, digital art, realistic\",\n",
    "    negative_prompt=\"3d, cartoon, lowres, bad anatomy, text, error, fewer digits, cropped, worst quality, \\\n",
    "        low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, \\\n",
    "        bad art, poorly drawn face, body out of frame, mutated, extra limbs, extra legs, extra arms,\\\n",
    "        imperfect, extra hands, bad hands,\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332fa83",
   "metadata": {},
   "source": [
    "Again, adding a negative prompt does improve the results so, consider adding it to your generation params after you improvise your prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af006b2f",
   "metadata": {},
   "source": [
    "**Character with detailed Prompts**\n",
    "\n",
    "This prompt helps understand what language to use with model to paint amazing characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea95532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36297a84d024e68b5502fbb889957e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 0.jpg\n",
      "\tdetect 1 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/-lXz4meRXQjTn8wFtOt5cg==\n",
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 1.jpg\n",
      "\tdetect 1 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/-lXz4meRXQjTn8wFtOt5cg==\n",
      "Face detection model: retinaface_resnet50\n",
      "Background upsampling: True, Face upsampling: True\n",
      "[1/1] Processing: 2.jpg\n",
      "\tdetect 1 faces\n",
      "\n",
      "All results are saved in dreamshaper_5_baked_vae/-lXz4meRXQjTn8wFtOt5cg==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/-lXz4meRXQjTn8wFtOt5cg==/final_results/0.png\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/-lXz4meRXQjTn8wFtOt5cg==/final_results/1.png\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"dreamshaper_5_baked_vae/-lXz4meRXQjTn8wFtOt5cg==/final_results/2.png\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt=\"modelshoot style, close up, extremely detailed 8k wallpaper,\\\n",
    "        full shot body photo, beautiful artwork in the world, medieval armor,\\\n",
    "        professional majestic oil painting by Ed Blinkey, Atey Ghailan,by Jeremy Mann, Greg Manchess,\\\n",
    "        trending on ArtStation and CGSociety, Intricate, High Detail, Sharp, dramatic,\\\n",
    "        photorealistic painting by midjourney, rutkowski\",\n",
    "    negative_prompt=\"canvas frame, cartoon, 3d, disfigured, bad art, deformed, extra limbs,\\\n",
    "        close up, b&w, wierd colors, blurry, duplicate, morbid, mutilated, out of frame, extra fingers, \\\n",
    "        mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, ugly, blurry, bad anatomy,\\\n",
    "        bad proportions, extra limbs, cloned face, ugly, extra limbs, bad anatomy, gross proportions, \\\n",
    "        malformed limbs, missing arms, missing legs, extra arms, extra legs, mutated hands, fused fingers, \\\n",
    "        too many fingers, long neck, Photoshop, video game, ugly, tiling, deformed, cross-eye, body out of \\\n",
    "        frame, blurry, bad art, bad anatomy, 3d render\",\n",
    "    N=3,\n",
    "    upscale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad753e6c",
   "metadata": {},
   "source": [
    "## Paint your own images\n",
    "- Start with a simple prompt\n",
    "- Improvise and add more details\n",
    "- Add a negative prompt(from the above examples) to remove artifacts and improve overall quality.\n",
    "- Set ```upscale=True``` once you have your final results from the diffusion model, to get higher resolution images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt=\"\",\n",
    "    N=3,\n",
    "    upscale=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "diffusers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
