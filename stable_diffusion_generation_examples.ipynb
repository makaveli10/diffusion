{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc54c6f",
   "metadata": {},
   "source": [
    "# Stable Diffusion generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4146a4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".inner_cell div.text_cell_render {\n",
       "    font-size: 125%;\n",
       "    overflow: none;\n",
       "    border-left: 10px solid #ff903b;\n",
       "    background: #ffe5d0;\n",
       "    margin-left: -15px;\n",
       "    padding-left: 15px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pylab\n",
    "from smlai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b84eb9",
   "metadata": {},
   "source": [
    "# Stable Diffusion\n",
    "Generating photo-realistic images given any text input. In short, we provide a prompt which is a text description of a subject that we wish to create using the diffusion model. A prompt can be as simple as a single line of vague text or several lines of text, depending on how detailed we want the output to be. \n",
    "\n",
    "\n",
    "## Let's paint some landscapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd843af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet/diffusion_pytorch_model.safetensors not found\n"
     ]
    }
   ],
   "source": [
    "# loading a trained model\n",
    "model = load_model('jzli/DreamShaper-3.3-baked-vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78da4b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72de839bf12b4faf80da8a5de06da6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"DreamShaper-3.3-baked-vae/aSEa3qsBuDA2ZV6b0d5xyQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"DreamShaper-3.3-baked-vae/aSEa3qsBuDA2ZV6b0d5xyQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"DreamShaper-3.3-baked-vae/aSEa3qsBuDA2ZV6b0d5xyQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model, \n",
    "    \"self\", \n",
    "    prompt=\"a man standing on top of a lush green field next to a mountain covered \\\n",
    "    in clouds and a giant mountain in the background, landscape\", \n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9a281",
   "metadata": {},
   "source": [
    "### Negative Prompt\n",
    "Stable Diffusion allows us to input negative prompts to let the model know the elements we want to remove from the final output. For instance, lets remove the anime, cartoon effects and also make it high res so our negative prompt could be as simple as \"cartoon, anime, sketches, lowres\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617db7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d4ae4358154267b52f95a563ffebd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"DreamShaper-3.3-baked-vae/F1sUDhLuDIFOjjv2qqowxw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"DreamShaper-3.3-baked-vae/F1sUDhLuDIFOjjv2qqowxw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"DreamShaper-3.3-baked-vae/F1sUDhLuDIFOjjv2qqowxw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model, \n",
    "    prompt=\"a man standing on top of a lush green field next to a mountain covered \\\n",
    "    in clouds and a giant mountain in the background, landscape\", \n",
    "    negative_prompt=\"cartoon, anime, sketches, lowres\" ,\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26bf9ca",
   "metadata": {},
   "source": [
    "## Fantasy architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5205d0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet/diffusion_pytorch_model.safetensors not found\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"stablediffusionapi/deliberate-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54682b75",
   "metadata": {},
   "source": [
    "### Detailed prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2478b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160f6cc1f20941059c7a3dfee5bd3a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/qod_1mpvc4R_8yoFZqarOg==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/qod_1mpvc4R_8yoFZqarOg==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/qod_1mpvc4R_8yoFZqarOg==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model, \n",
    "    prompt=\"building, gothic in gothbuilding style, goth, horror, creepy, no humans, tree, scenery, \\\n",
    "            outdoors, fog, window, sky, forest, nature, cloud, house, bare tree\",\n",
    "    negative_prompt=\"[ng_deepnegative_v1_75t], (easynegative:1.1), NSFW, text, error, cropped, \\\n",
    "            worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, \\\n",
    "            username, blurry, out of focus, censorship, [out of frame], artist name, sketch, \\\n",
    "            comic, (worst quality:2), (low quality:2), (normal quality:2), lowres, ((monochrome)), \\\n",
    "            ((grayscale)), out of the image\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636beb5",
   "metadata": {},
   "source": [
    "Wait! these dont look very much like goth buildings? Stable diffusion should do better than this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564aa4c7",
   "metadata": {},
   "source": [
    "### Load a textual inversion model\n",
    "Textual inversion involves training the Stable Diffusion model to better recreate a set of image's distinct features when generating from the same model by functionally creating a brand new word token for the model.\n",
    "Using a [Goth Building textual inversion model](https://civitai.com/models/27912/goth-building-style-lord-of-the-rings-style-house-castle-or-landscape-gothbuilding?modelVersionId=33450) from civit ai\n",
    "\n",
    "Load models from [civitai](https://civitai.com/), a hobbithouse finetuned checkpoint.\n",
    "- Step 1: Download the textual inversion checkpoint\n",
    "- Step 2: Search and find the base model on [huggingface-hub models](https://huggingface.co/models). In this case, the base model is ```stablediffusionapi/deliberate-v2```\n",
    "- Step 3: Load with the mentioned base model\n",
    "- Step 4: Load textual inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8722baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "text_encoder/model.safetensors not found\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"stablediffusionapi/deliberate-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc375a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-downloaded goth textual inversion checkpoint\n",
    "model.load_textual_inversion('textual_inversion/deliberate_v2/gothbuilding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231a7f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5883b33cf8b249e9a1bf19a81a06db71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/w6KXGze_7mNDnkmmwGuF-g==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/w6KXGze_7mNDnkmmwGuF-g==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/w6KXGze_7mNDnkmmwGuF-g==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model, \n",
    "    prompt=\"building, gothic in gothbuilding style, goth, horror, creepy, no humans, tree, scenery, outdoors, \\\n",
    "        fog, window, sky, forest, nature, cloud, house, bare tree\",\n",
    "    negative_prompt=\"[ng_deepnegative_v1_75t], (easynegative:1.1), NSFW, text, error, cropped, worst quality, \\\n",
    "        low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, out of focus, \\\n",
    "        censorship, [out of frame], artist name, sketch, comic, (worst quality:2), (low quality:2), \\\n",
    "        (normal quality:2), lowres, ((monochrome)), ((grayscale)), out of the image\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9299b375",
   "metadata": {},
   "source": [
    "### A hobbit house textual inversion model\n",
    "A fantasy cottage in the style of Lord of The Rings or a [Hobbithouse](https://civitai.com/models/18978/better-hobbit-house-fantasy-cottage-in-the-style-of-lord-of-the-rings)\n",
    "Lets add hobbithouse to our stable diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2564e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-downloaded hobbithouse textual inversion model\n",
    "model.load_textual_inversion('textual_inversion/deliberate_v2/hobbithouse.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "627ad59d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (121 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['- winning image, highly detailed, 1 6 k, cinematic perspective, ( ( video game environment concept art style ) ), pretty colors, cinematic environment, ( flat style : 1. 3 ), illustration, behance']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e1a0946c0a4d63a90a5fb5e531b078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/4wRu5pfvXICtL-VCS6H6-A==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/4wRu5pfvXICtL-VCS6H6-A==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"deliberate-v2/4wRu5pfvXICtL-VCS6H6-A==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, \n",
    "         prompt=\"(Vector image:1.3) of (Award-winning photograph), (one hobbithouse-37500 in the middle of the \\\n",
    "             forest), (Low-angle perspective), (natural lighting), (Wide-angle lens capturing scenery), \\\n",
    "             hidden objects games, video game concept art, (8K Unity wallpaper), fine details, award-winning \\\n",
    "             image, highly detailed, 16k, cinematic perspective, ((video game environment concept art style)), \\\n",
    "             pretty colors, cinematic environment,(Flat style:1.3), Illustration, Behance\", \n",
    "         negative_prompt=\"ng_deepnegative_v1_75t, ugly, duplication, duplicates, mutilation, deformed, \\\n",
    "             out of frame, grainy, blurred, blurry, writing, calligraphy, signature, text, watermark, bad art, \\\n",
    "             neg_facelift512, worst quality, low quality, medium quality, deleted, lowres, comic,(Watermark:1.5),\\\n",
    "             (Text:1.3), watermark, signature, frame,watermark,signature\",\n",
    "         N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338cd31",
   "metadata": {},
   "source": [
    "## Fantasy Art\n",
    "Painting some fantasy art with pre-trained models\n",
    "Model loaded from civit ai [VinteProtogenMix](https://civitai.com/models/5657/vinteprotogenmix?modelVersionId=23690)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fcfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_civit_ckpt('./vinteprotogenmix_V20.safetensors', model_name='vinteprotogenmix_V20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c68c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6038c030869432cb8282cf50b77b78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/2VphNVjcy12eTb8CRoE-sQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/2VphNVjcy12eTb8CRoE-sQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/2VphNVjcy12eTb8CRoE-sQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt='a bird sitting on a branch with pink flowers, cgi art, red-yellow colors, hyperrealistic sparrows, pinterest, photorealistic artstyle, in rich color, an ai generated image, very detailed portrait, singing for you, breathtaking render',\n",
    "    negative_prompt='ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, \\\n",
    "        extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, \\\n",
    "        signature, cut off, draft,',\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d150464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99040f2afbc4c1b8dc203dbe9d2bd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/8K7m-yE44YHsenXwUIz68w==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/8K7m-yE44YHsenXwUIz68w==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/8K7m-yE44YHsenXwUIz68w==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt='a living room filled with lots of furniture and decor, inspired by Josephine Wall, instagram contest \\\n",
    "        winner, vibrant autumn colors, string lights, underground room, light - brown wall, cushions, arab inspired, \\\n",
    "        cramped new york apartment, by Daniel Ljunggren, cottagecore hippie, thisset style',\n",
    "    negative_prompt='ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, \\\n",
    "        extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, \\\n",
    "        signature, cut off, draft,',\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f712b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917e3235bc394799ba786caf91d6808b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/BwyC6JZNaDZjesqVDazc4A==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/BwyC6JZNaDZjesqVDazc4A==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/BwyC6JZNaDZjesqVDazc4A==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    prompt='a man dressed as a bull with a machine gun, litrpg novel cover, character art. sci-fi. cover art, \\\n",
    "        male soldier in the forest, discord profile picture, ultra realistic photography, zulu, fractal ceramic \\\n",
    "        armor, battle ready, 2 k aesthetic, 2077, biopunk, photoreal details',\n",
    "    negative_prompt='ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, \\\n",
    "        extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, \\\n",
    "        signature, cut off, draft,',\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6c318cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeae6e7612ce4a10b724945a80e3b391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/5Cz1H0G2SrR7aPTyE3SVnQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/5Cz1H0G2SrR7aPTyE3SVnQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/5Cz1H0G2SrR7aPTyE3SVnQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the seed to reproduce results\n",
    "generate(\n",
    "    model,\n",
    "    prompt='a man dressed as a bull with a machine gun, litrpg novel cover, character art. sci-fi. cover art, \\\n",
    "        male soldier in the forest, discord profile picture, ultra realistic photography, zulu, fractal ceramic \\\n",
    "        armor, battle ready, 2 k aesthetic, 2077, biopunk, photoreal details',\n",
    "    negative_prompt='ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, \\\n",
    "        extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, \\\n",
    "        signature, cut off, draft,',\n",
    "    seed=932813817,\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e57a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744ad8f1c0d84596b2db34e87e44794c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/DE3uRWk-Oz7ZT7ShUnuldQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/DE3uRWk-Oz7ZT7ShUnuldQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"vinteprotogenmix_V20/DE3uRWk-Oz7ZT7ShUnuldQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the seed to reproduce results\n",
    "generate(\n",
    "    model,\n",
    "    prompt='a dog with glasses and a chain around its neck, radiant nebula colors, vfx movie, marvel art, \\\n",
    "        highly photographic render, pitbull, illustration iridescent, heavy metal artwork, his head covered in \\\n",
    "        jewels, similar to the bifrost, conceptart. com, avatar image, genie, love death and robots',\n",
    "    negative_prompt='ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, \\\n",
    "        extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, \\\n",
    "        signature, cut off, draft,',\n",
    "    seed=4288483519,\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9f9cf",
   "metadata": {},
   "source": [
    "## Cars\n",
    "Some aesthetic cars, using another checkpoint from civit-ai for [Battle Cars](https://civitai.com/models/35438/battle-cars)\n",
    "\n",
    "Steps to convert a ```lora``` checkpoint:\n",
    "- Download the checkpoint from civit ai.\n",
    "- Find the name of base model mentioned in the image generation data. Search and download from [huggingface-hub models](https://huggingface.co/models).\n",
    "- Convert to diffusers format and load using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a civit ai lora safetensor model\n",
    "# convert_civit_lora_safetensors_to_diffusers(\n",
    "#     'abyssorangemix3AOM3_aom3a1b.safetensors', # base model\n",
    "#     'battleCars_v2.safetensors',  # checkpoint\n",
    "#     'battleCars_v2'  # name of the dir\n",
    "# )\n",
    "\n",
    "\n",
    "# load your model from the dir you saved in\n",
    "# model = load_model('battleCars_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1070dd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet/diffusion_pytorch_model.safetensors not found\n"
     ]
    }
   ],
   "source": [
    "# We have already converted and saved the battle-cars checkpoint\n",
    "model = load_model(\"makaveli10/battleCars_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8f638e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c1295d81e5442f8a4dce7c6879d86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"battleCars_v2/qngEEqUD0YZuGN-X-S7sNw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"battleCars_v2/qngEEqUD0YZuGN-X-S7sNw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"battleCars_v2/qngEEqUD0YZuGN-X-S7sNw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, \n",
    "         \"(masterpiece, best quality:1.1), ultra-detailed, (battlecar:1.1), (humvee:1.08), (painted glossy red:1.05), vehicle focus, no humans, car, wheel, tire, debris, splash, sparks, electricity, glowing, water, dirty\", \n",
    "         negative_prompt=\"(worst quality, low quality:1.3), monochrome, blurry, license plate, english text, lowres, low detail, artist name, signature, watermark\",\n",
    "         N=3, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcc1fc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3633b6507a904f558c3ffff7fa08ce6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"battleCars_v2/ZJv18IE68EQC-a84T1Sq3A==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"battleCars_v2/ZJv18IE68EQC-a84T1Sq3A==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"battleCars_v2/ZJv18IE68EQC-a84T1Sq3A==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, \n",
    "         \"(masterpiece, best quality:1.1), ultra-detailed, (battlecar:1.1), (limousine:1.08), (painted turquoise, colorful:1.05), vehicle focus, no humans, car, wheel, tire, debris, splash, water, dirty\", \n",
    "         negative_prompt=\"(worst quality, low quality:1.3), monochrome, blurry, license plate, english text, lowres, low detail, artist name, signature, watermark\",\n",
    "         N=3, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0cd99",
   "metadata": {},
   "source": [
    "## Lowkey and smooth paintings\n",
    "Loading the ```lora``` checkpoint from civit-ai [LowRA model](https://civitai.com/models/48139/lowra?modelVersionId=52753), this checkpoint uses ```stablediffusionapi/deliberate-v2``` as the base model which is mentioned on the civit ai model information. So, lets' convert this checkpoint to diffusers and generate some cool images:\n",
    "- Download the ```lora``` checkpoint from civit-ai.\n",
    "- convert civit-ai ```lora``` checkpoint to diffusers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the checkpoint to diffusers\n",
    "model = convert_civit_lora_safetensors_to_diffusers(\n",
    "    'stablediffusionapi/deliberate-v2',  # base model, a huggingface model\n",
    "    'lowra_v10.safetensors',             # lora checkpoint from civit ai\n",
    "    'lowra',                             # name of the dir to save the converted diffuser model\n",
    "    from_ckpt=False)                     # true if the base model is also a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54e4a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the converted model\n",
    "model = load_model('lowra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2d057d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', mashrooms']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d9b1c677c4429ab8c885ef2f386122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/NuWNHpDbmGFxk2PG6Vandg==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/NuWNHpDbmGFxk2PG6Vandg==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/NuWNHpDbmGFxk2PG6Vandg==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    \"<lora:lowra_v10:0.8> inconceivable and spectacular a scene of emergence of a figure from the glowing cloud, fractal nebula threads, cosmic entities, celestial, cosmic, vibrant and vivid, swirls, twirling, unrealistic, high contrast, symbolism, magical, mystical, mystifying, hyperrealistic, oversaturate, mashrooms\",\n",
    "    negative_prompt=\"(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2792c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d801e70146044cff83007688b50eb27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/zqq7kEgy1vC_3kCjaJXdSQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/zqq7kEgy1vC_3kCjaJXdSQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/zqq7kEgy1vC_3kCjaJXdSQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    \"<lora:lowra_v10:0.8> 250 gto, motion shot, dark theme, (hdr:1.2)\",\n",
    "    negative_prompt=\"(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, ugly, disgusting, blurry, amputation\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1634e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8cd67e9ddc44fe9d644a14ac4e605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/EeT6RqsaSUiWV47sJXGcKw==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/EeT6RqsaSUiWV47sJXGcKw==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"lowra/EeT6RqsaSUiWV47sJXGcKw==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    \"<lora:lowra_v10:0.6> dark an old chieftainman, (astronaut outfit:1.2), feathers headdress, medium shot\",\n",
    "    negative_prompt=\"(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, \\\n",
    "        missing limb, floating limbs, (mutated hands and fingers:1.4), disconnected limbs, mutation, mutated, \\\n",
    "        ugly, disgusting, blurry, amputation\",\n",
    "    N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320844de",
   "metadata": {},
   "source": [
    "## Animated Art\n",
    "Using a trained checkpoint from civit-ai - [526mix-animated](https://civitai.com/models/35893/526mix-animated)\n",
    "- Download the checkpoint.\n",
    "- And load using ```load_civit_checkpoint```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3345dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step key not found in model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = load_civit_ckpt('526mixAnimated_v1.safetensors', '526Mix-Animated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fedbc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f287acdb08840a3a25f3e2e5f7d4f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"526Mix-Animated/PA2fHQt4yj3S2pq_BSBAUQ==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"526Mix-Animated/PA2fHQt4yj3S2pq_BSBAUQ==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"526Mix-Animated/PA2fHQt4yj3S2pq_BSBAUQ==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model,\n",
    "        \"closeup (1980's dark sci-fi anime style)+++ futuristic black ops soldier aiming an energy rifle in battle, background smoke, combat stance, epic composition, particles, action scene\",\n",
    "        negative_prompt=\"desaturated---, pixelated---\",\n",
    "        N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67e2cffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c1f8004a1a44259997c29ff0066c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img style=\"float:left; width: 32%; margin:5px;\" src=\"526Mix-Animated/LD0yykWfTFxfOJ69Dd0IWA==/0.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"526Mix-Animated/LD0yykWfTFxfOJ69Dd0IWA==/1.jpg\" /><img style=\"float:left; width: 32%; margin:5px;\" src=\"526Mix-Animated/LD0yykWfTFxfOJ69Dd0IWA==/2.jpg\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model,\n",
    "        \"2000's realistic anime style closeup shot of a middle-aged gruff detective eating ramen at a street \\\n",
    "        vendor in the rain, dark, after hours, cyberpunk city raining in background, vivid colors, neon signs\",\n",
    "        negative_prompt=\"umbrealla, umbreallas\",\n",
    "        N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21460a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "diffusers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
